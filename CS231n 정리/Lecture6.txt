===Training Neural Networks 1====
*Activation Functions 
-Sigmoid : negative value면 거의 0에 가까울 것이고 positive 하면 1에 가까울 것. 이건 firing rate of neuron이랑 비슷하기 때문에 유명함. 

<3 Problems>
 1) Saturated neurons "kill" the gradients: 시그모이드를 거친 값이 많이 negative이면 거의 gradient가 0에 가까움. 학습이 되지 않는다. 
 2) "Sigmoid outputs are not zero-centered" : 만약 뉴런 x에 대한 input이 항상 positive라면 w의 gradient가 어떻게 될까? 모든 x가 다 positive이면 all positive이거나 항상 all negative일 것...항상 같은 방향으로 움직인다는 것이다. 항상 increase the weight일 것. 이건 매우 inefficient gradient update다. zig zag path... 

 3) exp() is a bit compute expensive: exponential이 computationally 좀 cost가 많이 든다. 

-Tanh(x): squashes numbers to range[-1,1], zero centered(nice). 하지만 똑같이 flat 한 부분때문에 saturated된다. 
 
- ReLU: f(x) = max(0,x)     does not saturate (in + region) 
시그모이드나 tanh보다 6배 정도 빠르고 neuroscience experiment 에서 더 비슷한 결과를 보인다. 또 computation도 쉬운 편이다.
   -문제점: not 0 centered, an annoyance(양수에서는 saturation이 없지만 negative 에서는 not the case). x가 0보다 작을 때 기울기가 0이기 때문에 vanishing gradient 문제가 발생한다. 
   - 데이터 클라우드에서 멀리 떨어진 neuron이 있으면 죽어버린다.. dead ReLU 에서는 절대 작동 안 할거고 update 안 될 것... 가중치 초기화 할때 운 나쁘게 dead ReLU에서 시작하는 경우, 그리고 learning rate가 클 때는 뉴런이 조금씩 변화하다가 dead ReLU 존으로 나가버려서 학습을 하지 못하는 경우.. 사실 ReLU 에서 죽는 경우가 허다하다. 10퍼센트 정도. 그래서 사람들은 ReLU neuron을 0.01로 시작하면 도움된다고 한다. 

 -Leaky ReLU: does not saturate, 그래서 gradient가 죽는 경우가 발생하지 않는다. computationally efficient. 

 -Parametric Rectifier, PReLU: f(x) = max(alpha x, x). 0.01대신에 알파 값을 넣는다. 알파라는 것 자체도 학습을 하겠다는 게 이 특징이다. 

 -Exponential Linear Units(ELU): x가 0보다 클 때는 x, x보다 작을 때는 alpha(exponential(x)-1) .  ReLU의 모든 장점을 가지면서 zero mean output에 가까운 장점을 가진다. 하지만 exponential 연산이 비싸기 때문에 연산하는데 무리가 갈 수 있다. 

 - Maxout "Neuron" : generalizes ReLU and Leaky ReLU. 문제는 파라미터가 2배가 된다는 것이다. 연산이 2배로 증가하게 되는 것..

=> ReLU를 사용하는게 제일 좋다. 다만 learning rate를 선정할 때 조심해라. 
 --시그모이드는 더이상 쓰지 않는게 좋지만 LSTM에서는 쓴다. 

**Data Preprocessing
Step1: preprocess the data. 
1) zero-centering: 각각에 대해 전체 평균 값을 빼주는 것. 만약 모든 neuron 이 positive라면 optimization이 제대로 되지 않기 때문이다. 
2) Normalized data: 표준편차로 나눠주므로서 수행을 한다. 예를 들어 -1에서 1까지 특정 범위 안에 들어오게 하는 것이다. 모든 feature가 same range안에 있고 contribute equally하기 위함이다. 그치만 이미지에서는 상대적으론 normalize 하지 않는다. 이미지는 이미 픽셀이라는 범위 안에 들어가 있는 거기 때문이다.
PCA는 차원을 줄이는 거고 whitening은 중복성을 줄일 수 있도록 squashing해주는 것....근데 이미지에 대해서는 둘 다 쓰이지 않는다. 


**이미지에 있어서는 zero-centered하는 방법만 신경쓰면 된다. 
- 하는 방법?: 예를 들어 CIFAR-10 을 사용할 때 (32,32,3) 행렬로 부터 평균 이미지를 각각 빼주는 것. 또 다른 방법은  Subtract per-channel mean (e.g. VGGNet). channel 별로 mean을 만드는 것. 두 번째 방법이 훨씬 편하다. 

**Weight Initialization (가중치 초기화) 
1. small random numbers (0의 평균을 가지고 0.01의 표준편차를 가지는 가우시안 정규 분포 형태로 구성) 
 - 네트워크가 작으면 상관없지만 deeper 해질 수록 문제가 될 수 있다. 
 예) 10 layer net with 500 neurons on each layer, using tanh non-linearities (-1에서 1값으로 나오게)
 - mean은 0으로 잘 수렴하지만 문제는 std가 매우 급작하게 적어진다. w와 x가 계속 곱해질 수록 숫자가 적어지기 때문에 activation이 거의 0에 가깝게 된다. 
 Q. backwrd pass에서는 gradients가 어떻게 생겼을까? 
  -> dw1 = x*dw2 로 나타남. upstream gradient 와 weight를 곱해주는데 w를 또 계속 곱해야 하기 때문에 gradient가 거의 0 으로 수렴하게 된다. 

2. 만약 표준편차를 1에 가깝게 한다면 어떻게 될까?
-거의 모든 layer가 -1 또는 1로 saturated된다. 그래서 loss값이 거의 변하지 않음.


**그러면 어떻게 초기화 하는게 나올까?
1) "Xavier Initialization" 2010년 발표됨 
- 가중치 처음에 할 때 랜덤하게 정의하고 노드의 계수의 square root로 나눠준다.   
variance of input이 variance of output과 같게 맞춘다. 만약 input이 small하다면 weight를 더 크게 하는 등.. 
 -> 히스토그램에서 보다시피 saturation이 발생하지 않음. 
 -> tanh에는 잘 되지만 만약 activation을 ReLU를 쓰면 문제가 발생한다.  왜냐면 거의 반을 없애기 때문이다. 그래서 variance가 제대로 나오지 않아서 다시 distribution이 collapse된다. 

-> 이를 해결하기 위해 나온 것이 2015년 "Kaiming He" 가 발표한 방법, 바로 input을 2로 나눠주는 것. (이 아니라 루트 2를 곱해주는 방법이라고 한다) 

사실 weight initialization은 지금도 활발하게 연구가 진행되고 있다.. 다른 논문도 읽어보면 좋다.


근데 너무 weight initialization에 의존하지 않아도 되는게 있다. 바로 Batch Normalization. Vanishing Gradient문제가 발생하지 않도록 하는 대표적인 방법. 간접적인 방법이 아니라 학습 자체를 안정적으로 하겠다는 근본적인 방법을 제시하였다. 각 layer를 거칠때마다 이것들을 normalize를 해준다.

**Batch Normalization
-정규화를 해준다.  
1. compute the empirical mean and variance independently for each dimension. 
 -minibatch를 뽑아서 이 배치의 평균과 분산을 뽑아서 정규화 한다. FC -> Batch Normalization -> tanh ... 
 - 두 단계로 나눠서 보면 첫 번째는 normalize를 하고 두 번째는 정규화를 다시 조정할 수 있게 한다. 감마가 normalize한 것에 대해 scaling하는 변수가 된다. 베타는 shifting함... 감마와 베타는 학습을 통해 되고 이 normalize를 어느정도 할것인가 조정할 수 있다. (scale? shift?) 
- learning rate가 higher 해도 괜찮다. 
- 초기화에 너무 의존하지 않도록 해준다. 
- 그 자체가 regularization을 하는 효과를 준다. 같은 input에 대해서 같은 output을 나오니까..? )그래서 나중에 배우게 되는 dropout은 batch normalization하게 되면 잘 사용하지 않는다. 

training 할 경우에는 batch를 기준으로 구하게 되고 test할 때는 전체에 대해 mean과 variance를 기준으로 해서 batch normalize해준다. 


=================
**Babysitting the Learning Process
-how do we adjust the parameters to get the good learning result? 
Preprocessing -> Choosing the architecture -> Double Check that the loss is reasonable (sanity check 등을 통해서..) 

본격적인 학습을 시작하기 전에..... 
 - 전체 중에 20개만 돌려봄. 그리고 regularization을 꺼주고 업데이트 방식은 sgd로 함. 이렇게 학습 데이터 중에 일부만 사용하면 반드시 오버 핏팅이 일어나야한다. loss가 거의 0에 가깝고 train accuracy가 1.0에 근접하게....만약 오버핏팅이 일어나지 않으면 문제가 있다는 것이고 일어난다면 weight 가 잘 학습된다는 것으로 본다. 
-또 learning rate를 너무 작게 설정하면 loss는 거의 변화가 없다. 그런데 training accuracy가 증가를 하고 있다. 대체 왜? 
  -> 처음에 시작할 때 diffuse score로 시작하는데 그 상태에서 학습을 시작한다. 그러면 diffuse score가 약간 변화를 하게 될 것이고 동일한 loss를 가지게 된다. 하지만 score에 대해서는 계속 변화하고 있기 때문에 (?)
-만약 learning rate가 너무 크다면 그 때는 cost가 NaN이 되버린다...


**Hyperparameter Optimization
1) Cross-validation strategy: 
 tip) 학습 한 후 ocst를 보라. original cost보다 3배 더 크다면 그건 right direction으로 가고 있지 않다는 뜻이다. 
 - It's best to optimize in log space!! 
 - 그리고 learning rate의 범위 정해놨는데 더 좋은 rate가 있다면 범위를 제대로 설정하지 않았다는 것이니까 제대로 봐야한다. 

**Hyperparameters to play with: 
 network architecture, learning rate, decay schedule, update type, regularization.... 
 거의 몇 천 개의 hyperparameter로 실험한다.
그 외 보는 법 여러가지 보여줬당. 


==================
Question: loss가 변화하지 않아도 왜 accuracy가 늘어나느것인가? learning하는 방향은 옳게 가고 있기 때문이라? 



























 
