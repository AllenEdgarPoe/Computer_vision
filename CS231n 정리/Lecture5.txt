=====Convolution Neural Networks=======

FC(fully connected layer) : stretch matrix
 ex) 32*32*3 -> 3072*1 
 Wx.size = (10,3072) 왜냐면 레이블의 갯수는 10이니까. 즉, output 10개를 만들어내는 layer. 

*Convolution Layer: input이 FC와 다르게 원형의 크기를 보존한 상태로 둔다. weight값은 filter라고 불리게 되는데 filter의 깊이(갯수)는 input값과 같다. 
 ex) input.shape = (32,32,3);  filter.shape = (5,5,3) 
그리고 필터가 input을 차례로 훝어가면서 x*w 값 즉 score를 만듬. 다양한 필터를 사용하기 때문에 activation map은 여러 가지가 나올 수 있다. 예를 들어 필터의 갯수가 6이라면 다음 레이어의 크기는 (28,28,6) 

-필터의 갯수가 많을 수록 더 복잡하고 정교한 특징을 추출할 수 있다. 

-CNN은 conv-activation-pool 이 연달아 나오는 형식.. 


*Padding 
: 필터의 크기가 클수록, 이미지의 크기는 줄어들게 된다. 그러므로 가장 자리에 어떤 숫자들을 채워 넣어서 사이즈를 보존하는 방법. zero padding 등이 있음. 


**Output Size 계산하기 
(input size - filter size + 2*Padding)/stride  + 1. stride는 얼마나 필터를 간격을 두고 weight곱을 할 것인가 임. 

파라미터의 갯수: (필터의 크기*필터의 크기*input의 깊이)*필터의 갯수 + 필터의 갯수 

필터의 겟수는 일반적으로 32, 64, 128, 512 이런식으로 늘려서 사용한다. 

??필터의 크기가 1인 Convnet은 뭐지? -> 깊이를 더 깊게 쌓기 위한 방법 중 하나라고 한다. input이 (filter size, height, width) 면 크기가 1이고 size가 n인 필터를 지나게 되면 (n, height, width)가 된다. 따라서 n>filter size면 더 깊이 있는 레이어가 되는 것 


***Pooling
-activation map의 크기를 downsampling, 즉 이미지의 크기를 줄이는 역할이다. padding을 한 것은 크기 보존 뿐만 아니라 이미지의 부분적인 특징을 살리기 위한 것이였다. 왜냐면 계속 convolution하다보면, 가장자리 부분의 특징이 잘 살아나지 않기 때문이다. 하지만, 이번에 하는 pooling은 이미지의 특정 부분을 잘라내는 것이 아니라, 사진 전체 부분을 유지한 상태로 픽셀만 줄이는 것임.... 
-왜 풀링을 하는가?? : filter의 갯수가 늘어날수록 activation map의 깊이가 너무 깊어지게 되고 그러면 계산하는데 너무 오래 걸림...그래서 pooling으로 크기를 줄여주는 것. 
-풀링의 방법에는 max pooling 등이 있다. 










