[previously...]
svm hinge loss, softmax cross-entropy loss 등을 배움. 
Optimization 방법: Gradient Descent 


====역전파법 Backpropagation & 인공신경망====
computational graph로 optimization 표현.. 복잡한 비선형 수식일 수록 그래프가 매우 복잡해지고 연산량도 많아진다. Alexnet같은 미친 갯수의 층을 가진 컨볼루션을 하나 하나 다 미분하고 있으면 analytic이라도 시간이 엄청나게 많이 걸릴 수 밖에 없다. 그래서 나온게 backpropagation!

*Backpropagation
- 입력값이 x,y,z 일 때, 이 x,y,z 값이 마지막 output 값에 끼치는 영향력을 알고 싶으면 편미분 한다. 이때 Chain Rule을 따라서 하면 쉬움 
-input이 x,y라면 f라는 함수에다 넣어서 output이 z다. z를 x로 미분한 값과, z를 y로 미분한 값을 얻고, 끝으로 Loss값을 계산. (forward pass)
 => Backpropagation: loss를 각각 x,y로 미분한 값을 구하려면, z를 x,y로 미분한 값과 chain rule을 사용해서 구한다. 
(backward pass)

*Sigmoid Function
: 미분 연산량이 너무나 많기 때문에 한 방에 미분할 수 있게 방법 고안. 왜냐면 sigmoid function의 미분값은 (1-sigmoid(x))*sigmoid(x)이기 때문에 죄다 한 번에 미분 가능. 


*add gate(덧셈), mul gate(곱셈), max gate(최대값) 
-> 이 게이트의 backward pass할 때: add gate는 이미 가지고 있던 gradient를 각각의 노드에 분배해줌. mul gate는 현재의 gradient를 각각 숫자에 곱해서 바꿔줌. max gate는 그냥 더 큰 쪽에만 gradient를 그대로 놔주고 반대쪽은 pass. 

*Gradients add at branches 
- 노드 하나에서 다른 노드 두 개로 모두 이어졌을 때, backpropagation을 수행하면 저 뒤에 두 개의 노드에서 오는 미분 값을 더함. 반대로 앞의 노드가 하나라도 바뀌면 뒤의 노드는 모드 바뀜...chain적이다. 


**인공 신경망
: 인간의 신경망을 따라 만든 거지만..실상은 거의 다 다르다. 선형 분류기에서는 score를 그냥 f = W*x 로 계산 했는데 더 정확하게 하기 위해서는 다른 가중치 weight값으로도 계산 가능해야 한다. 왜냐면 더욱 더 다양한 특징들을 잡아서 분류할 수 있어야 하기 때문이다. 
***비선형 (non-linearity) 를 위해서 activation function에 ReLU, Sigmoid, ELU 등을 씀.. 선형은 XOR문제 등 분류할 수 없는 경우가 있지만 비선형은 이론적으론 모든 것을 분류할 수 있다. 


*Fully-connected Layer: 모든 w값들이 다음의 모든 w에 영향을 미치는 레이어 



















